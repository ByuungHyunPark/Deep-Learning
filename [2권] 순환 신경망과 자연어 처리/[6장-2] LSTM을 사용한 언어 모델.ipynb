{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-4 LSTM을 사용한 언어 모델\n",
    "Time LSTM 계층을 구현해봤으니, 본래 주제인 __언어 모델__을 구현할 차례입니다. <br>\n",
    "여기서 구현하는 __언어 모델__은 앞장에서 구현한 언어 모델과 거의 같습니다. <br>\n",
    "[그림 6-26]에서 보듯, 앞 장에서는 Time RNN 계층이 차지하던 부분이 Time LSTM 계층으로 바뀌었는데, 이것이 유일한 차이입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.time_layers import *\n",
    "from common.base_model import BaseModel\n",
    "\n",
    "import pickle\n",
    "\n",
    "# https://github.com/WegraLee/deep-learning-from-scratch-2/blob/master/ch06/rnnlm.py\n",
    "class Rnnlm(BaseModel):\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        # 가중치 초기화\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.lstm_layer = self.layers[1]\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        score = self.predict(xs)\n",
    "        loss = self.loss_layer.forward(score, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.lstm_layer.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Rnnlm 클래스에는 Softmax 계층 직전까지를 처리하는 predict() 메서드가 추가되었습니다. <br>\n",
    "__이 메서드는 7장에서 수행하는 문장 생성에 사용됩니다.__ 그리고, 매개변수 읽기/쓰기를 처리하는 load_params()와 save_params() 메서드도 추가되었습니다. 나머지는 앞 장의 SimpleRnnlm 클래스와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___이제 이 신경망을 사용해 PTB 데이터셋을 학습시켜보겠습니다.___ <br>\n",
    "이번에는 PTB 데이터셋의 훈련데이터 전부를 사용해 학습합니다. (앞 장에서는 PTB 데이터셋의 일부만 이용했습니다). <br>\n",
    "학습을 위한 코드는 다음과 같습니다. (ch06/train_rnnlm.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 414 | 시간 0[s] | 퍼플렉서티 10000.07\n",
      "| 에폭 1 |  반복 51 / 414 | 시간 29[s] | 퍼플렉서티 1699.51\n",
      "| 에폭 1 |  반복 101 / 414 | 시간 57[s] | 퍼플렉서티 817.76\n",
      "| 에폭 1 |  반복 151 / 414 | 시간 86[s] | 퍼플렉서티 588.21\n",
      "| 에폭 1 |  반복 201 / 414 | 시간 116[s] | 퍼플렉서티 494.47\n",
      "| 에폭 1 |  반복 251 / 414 | 시간 145[s] | 퍼플렉서티 461.44\n",
      "| 에폭 1 |  반복 301 / 414 | 시간 177[s] | 퍼플렉서티 387.40\n",
      "| 에폭 1 |  반복 351 / 414 | 시간 207[s] | 퍼플렉서티 362.72\n",
      "| 에폭 1 |  반복 401 / 414 | 시간 237[s] | 퍼플렉서티 322.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning: Glyph 48152 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning: Glyph 48373 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning: Glyph 54140 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning: Glyph 54540 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning: Glyph 47113 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning: Glyph 49436 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning: Glyph 54000 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning: Glyph 48152 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning: Glyph 48373 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning: Glyph 54140 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning: Glyph 54540 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning: Glyph 47113 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning: Glyph 49436 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning: Glyph 54000 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "퍼플렉서티 평가 중 ...\n",
      "234 / 235\n",
      "테스트 퍼플렉서티:  296.54361078424716\n"
     ]
    }
   ],
   "source": [
    "from common.optimizer import SGD\n",
    "from common.trainer import RnnlmTrainer\n",
    "from common.util import eval_perplexity\n",
    "from dataset import ptb\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "batch_size = 64\n",
    "wordvec_size = 100\n",
    "hidden_size = 100  # RNN의 은닉 상태 벡터의 원소 수\n",
    "time_size = 35     # RNN을 펼치는 크기\n",
    "lr = 20.0\n",
    "max_epoch = 1\n",
    "max_grad = 0.25\n",
    "\n",
    "# 학습 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_test, _, _ = ptb.load_data('test')\n",
    "vocab_size = len(word_to_id)\n",
    "xs = corpus[:-1]\n",
    "ts = corpus[1:]\n",
    "\n",
    "# 모델 생성\n",
    "model = Rnnlm(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = SGD(lr)\n",
    "trainer = RnnlmTrainer(model, optimizer)\n",
    "\n",
    "# 기울기 클리핑을 적용하여 학습\n",
    "trainer.fit(xs, ts, max_epoch, batch_size, time_size, max_grad,\n",
    "            eval_interval=50)\n",
    "trainer.plot(ylim=(0, 500))\n",
    "\n",
    "# 테스트 데이터로 평가\n",
    "model.reset_state()\n",
    "ppl_test = eval_perplexity(model, corpus_test)\n",
    "print('테스트 퍼플렉서티: ', ppl_test)\n",
    "\n",
    "# 매개변수 저장\n",
    "model.save_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞장과의 차이는 __기울기 클리핑__의 진행 여부, 빠른 진행을 위해 에폭수는 줄였음 <br>\n",
    "에폭 수 늘려도, 퍼플렉서티가 만족스러운 결과는 아니므로, 아래에서 성능을 한층 더 개선할 계획이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.5 RNNLM 추가 개선\n",
    "이번 절에서는 현재의 RNNLM의 개선 포인트 3가지를 설명합니다. 그리고 그 개선들을 구현하고, 마지막에는 실제로 얼마나 좋아졌는지를 평가해보겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.5.1 LSTM 계층 다층화\n",
    "RNNLM으로 정확한 모델을 만들고자 한다면 많은 경우 LSTM을 깊게 쌓아(계층을 여러겹 쌓아)효과를 볼 수 있다.\n",
    "![](img/fig-6-29.png)\n",
    "\n",
    "지금까지 우리는 LSTM 계층을 1층만 사용했지만 이를 2층, 3층 식으로 여러겹 쌓으면 언어 모델의 정확도가 향상되리라 기대할 수 있습니다. <br>예컨대 LSTM을 2층으로 쌓아 RNNLM을 만든다고 하면 위 [그림6-29]처럼 됩니다. <br>\n",
    "\n",
    "[그림6-29]는 LSTM계층을 두 개 쌓은 모습입니다. 이때 첫 번째 LSTM 계층의 은닉 상태가 두번째 LSTM 계층에 입력됩니다. 이와 같은 요령으로 LSTM 계층을 몇층이라도 쌓을 수 있으며, 그 결과 더 복잡한 패턴을 학습할 수 있게 됩니다. 피드포워드 신경망에서 계층을 쌓는 이야기와 같다.  <br>\n",
    "\n",
    "PTB 데이터 셋의 언어모델은 보통 LSTM의 층 수를 2~4 정도로 할 때 좋은 결과를 얻는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.5.2 Dropout에 의한 과적합 억제\n",
    "LSTM 계층을 다층화 하면 시계열 데이터의 복잡한 의존관계를 학습할 수 있을 것이라 기대할 수 있습니다. 다르게 표현하자면, 층을 깊게 쌓음으로써 표현력이 풍부한 모델을 만들 수 있다. 그러나, 이러한 모델은 종종__과적합__을 일으킵니다.  <br>심지어는 RNN은 일반적인 피드포워드 신경망보다 쉽게 과적합이 발생한다.  <br>\n",
    "따라서 RNN의 과적합 대책은 중요하고, 현재도 활발하게 연구되는 주제이다. \n",
    "\n",
    "![](img/fig-6-30.png)\n",
    "\n",
    "드롭아웃은 무작위로 뉴런을 선택하여 선택한 뉴런을 무시한다. 무시한다는 말은 그 앞 계층으로부터의 신호 전달을 막는다는 뜻이다.<br>\n",
    "이 __무작위한 무시__가 제약이 되어 신경망의 일반화 성능은 개선한 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/fig-6-31.png)\n",
    "\n",
    "이 그림은 드롭아웃 계층에 활성화 함수 뒤에 삽입하는 방법으로 과적합 억제에 기여하는 모습이다. <br>\n",
    "RNN을 사용한 모델에서 드롭아웃 계층을 LSTM 계층의 시계열 방향으로 삽입하는 것인데, __이는 좋은 방법이 아니다.__\n",
    "\n",
    "![](img/fig-6-32.png)\n",
    "\n",
    "RNN에서 시계열 방향으로 드롭아웃을 학습 시 넣어버리면 시간이 흐름에 따라 정보가 사라질 수 있다 . <br>\n",
    "즉, 흐르는 시간에 비례해 드롭아웃에 의한 노이즈가 축적된다. <br>\n",
    "\n",
    "드롭아웃 계층을 깊이 방향(상하 방향)으로 삽입하는 방안을 생각해보자.\n",
    "\n",
    "![](img/fig-6-33.png)\n",
    "\n",
    "위처럼 구성하면 시간 방향으로 아무리 진행해도 정보를 잃지 않는다. 드롭 아웃이 시간축과는 독립적으로 깊이 방향에만 영향을 주는 것이다. <br>\n",
    "\n",
    "일반적인 드롭아웃은 시간방향에는 적합하지 않다. RNN의 시간 방향 정규화를 목표로 하는 방법이 다양하게 제안되다가 __변형 드롭아웃__이 제안되어 시간 방향으로 적용하는 데 성공했다.\n",
    "\n",
    "![](img/fig-6-34.png)\n",
    "\n",
    "계층의 드롭아웃끼리 마스크를 공유함으로써 마스크가 __고정__된다. 그 결과 정보를 잃게되는 방법도 __고정__ 되므로, 일반적인 드롭아웃 때와 달리 정보가 지수적으로 손실되는 사태를 피할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.5.3 가중치 공유\n",
    "언어 모델을 개선하는 아주 간단한 트릭 중 __가중치 공유__가 있습니다. weight tyig 을 직역하면 '가중치를 연결한다'이지만, 실질적으로는 [그림 6-35]에서 보듯 가중치를 공유하는 효과를 줍니다.\n",
    "\n",
    "![](img/fig-6-35.png)\n",
    "\n",
    "__가중치 공유는 Embedding 계층의 가중치와 Affine 계층의 가중치를 연결하는 기법이다.__ <br>\n",
    "두 계층이 가중치를 공유함으로써 학습하는 매개변수 수가 크게 줄어드는 동시에 정확도도 향상되는 일석이조의 기술이다.\n",
    "\n",
    "두 계층이 가중치를 공유함으로써 학습하는 매개변수 수가 크게 줄어드는 동시에 정확도도 향상되는 일석이조의 기술입니다. <br>\n",
    "\n",
    "그럼, 가중치 공유를 구현 관점에서 생각해봅시다. <br>\n",
    "> - 어휘 수를 $V$로, LSTM의 은닉 상태의 차원 수를 $H$로 가정\n",
    "> - 그러면 Embedding 계층의 가중치는 형상이 $V$x$H$이며\n",
    "> - Affine 계층의 가중치 형상은 $H$x$V$가 됩니다.\n",
    "> - 이때 가중치 공유를 적용하려면 Embedding 계층의 가중치를 전치하여 Affine 계층의 가중치로 설정하기만 하면 됩니다. 그리고 이 아주 간단한 트릭 하나로 훌륭한 결과를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.5.4 개선된 RNNLM 구현\n",
    "![](img/fig-6-36.png)\n",
    "\n",
    "위 그림[6-36]에서 보듯, 여기에서의 개선점은 다음 세가지이다. \n",
    "- LSTM 계층의 다층화(여기에서는 2층)\n",
    "- 드롭아웃 사용(깊이 방향으로만 적용)\n",
    "- 가중치 공유(Embedding 계층과 Affine 계층에서 가중치 공유)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/WegraLee/deep-learning-from-scratch-2/blob/master/ch06/better_rnnlm.py\n",
    "\n",
    "from common.time_layers import *\n",
    "from common.np import *  # import numpy as np\n",
    "from common.base_model import BaseModel\n",
    "\n",
    "\n",
    "class BetterRnnlm(BaseModel):\n",
    "    '''\n",
    "     LSTM 계층을 2개 사용하고 각 층에 드롭아웃을 적용한 모델이다.\n",
    "     아래 [1]에서 제안한 모델을 기초로 하였고, [2]와 [3]의 가중치 공유(weight tying)를 적용했다.\n",
    "     [1] Recurrent Neural Network Regularization (https://arxiv.org/abs/1409.2329)\n",
    "     [2] Using the Output Embedding to Improve Language Models (https://arxiv.org/abs/1608.05859)\n",
    "     [3] Tying Word Vectors and Word Classifiers (https://arxiv.org/pdf/1611.01462.pdf)\n",
    "    '''\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=650,\n",
    "                 hidden_size=650, dropout_ratio=0.5):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx1 = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh1 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b1 = np.zeros(4 * H).astype('f')\n",
    "        lstm_Wx2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_Wh2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b2 = np.zeros(4 * H).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, stateful=True),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeLSTM(lstm_Wx2, lstm_Wh2, lstm_b2, stateful=True),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeAffine(embed_W.T, affine_b)  # weight tying!!\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.lstm_layers = [self.layers[2], self.layers[4]]\n",
    "        self.drop_layers = [self.layers[1], self.layers[3], self.layers[5]]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs, train_flg=False):\n",
    "        for layer in self.drop_layers:\n",
    "            layer.train_flg = train_flg\n",
    "\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts, train_flg=True):\n",
    "        score = self.predict(xs, train_flg)\n",
    "        loss = self.loss_layer.forward(score, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        for layer in self.lstm_layers:\n",
    "            layer.reset_state()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
