{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고링크\n",
    "- https://velog.io/@dscwinterstudy/2020-02-02-0002-%EC%9E%91%EC%84%B1%EB%90%A8-pwk63r14ez#31-%EC%B6%94%EB%A1%A0-%EA%B8%B0%EB%B0%98-%EA%B8%B0%EB%B2%95%EA%B3%BC-%EC%8B%A0%EA%B2%BD%EB%A7%9D\n",
    "- https://asthtls.tistory.com/1058"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1자연어 처리(NLP - Natural Language Processing)\n",
    "- 자연어(Natural Language) : 인간의 언어\n",
    "- 자연어 처리 - 자연어를 처리하는 분야\n",
    "- 인간의 말을 컴퓨터에게 이해시키기 위한 기술\n",
    "\n",
    "##### 2.1.1 단어의 의미\n",
    "- 단어 : 의미의 최소 단위\n",
    "- 컴퓨터에게 이해시키는 데는 '단어의 의미'를 이해시키는 것이 중요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 시소러스(thesaurus)\n",
    "__유의어 사전__을 의미한다. <br>\n",
    "'뜻이 같은 단어(동의어)'나 '뜻이 비슷한 단어(유의어)' 가 한 그룹으로 분류되어 있다.\n",
    "![](img/thesaurus.png)\n",
    "\n",
    "또한 자연어 처리에 이용되는 시소러스에서는 단어 사이의 __상위와 하위__ 혹은 __전체와 부분__등, 더 세세한 관계까지 정의해둔 경우도 있다.<br><br>\n",
    "\n",
    "__밑의 그림처럼 각 단어의 관계를 그래프 구조로 정의한다.__<br>\n",
    "\n",
    "![](img/thesaurus2.png)\n",
    "\n",
    "\n",
    "__car__의 상위 개념으로 __motor vehicle__이라는 단어가 존재하며 하위 개념으로 __SUV, compact, hatch-back__등 구체적인 차종이 존재한다. <br><br>\n",
    "> 이처럼 __단어 네트워크를 이용하여 컴퓨터에게 단어의 관계를 가르쳐줄 수 있다.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1 WordNet\n",
    "자연어 처리 분야에서 가장 유명한 시소러스는 WordNet이다. <br>\n",
    "WordNet을 사용하면 유의어를 얻거나 __단어 네트워크__를 이용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2 시소러스의 문제점\n",
    "\n",
    "WordNet과 같은 시소러스에는 수많은 단어에 대한 동의어와 계츨 구조 등의 관계가 정의되어 있다. <br>\n",
    "그리고 이 지식을 이용하면 __단어의 의미__를 컴퓨터에 전달할 수 있다. <br.<br>\n",
    "\n",
    "하지만, 사람이 수작업으로 Labeling 하는 방식에는 문제들이 존재한다.\n",
    "- 시대 변화에 대응하기 어렵다. 신조어 혹은 의미 변화된 단어들을 바로 적용시키기 어렵다.\n",
    "- 사람을 쓰는 비용이 든다. 현존하는 영어 단어의 수는 1,000만 개가 넘으며, WordNet에 등록된 단어는 20만개 이상이다. \n",
    "- 단어의 미묘한 차이를 표현할 수 없다. 가령, 빈티지와 레트로의 의미는 같으나 용법의 차이가 존재한다.\n",
    "\n",
    "__위 문제점들을 피하기 위해 '통계 기반 기법'과 신경망을 사용한 '추록 기반 기법'을 알아볼 것이다.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 통계 기반 기법\n",
    "통계 기반 기법을 살펴보면서 말뭉치, 즉 대량의 텍스트 데이터를 이용할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1 파이썬으로 말뭉치 전처리하기\n",
    "자연어 처리에는 다양한 말뭉치가 사용되는데 예로는 구글 뉴스와 위키백과 등의 텍스트 데이터를 들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus :  [0 1 2 3 4 1 5]\n",
      "word_to_id :  {'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello.': 5}\n",
      "id_to_word :  {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello.'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower() # 소문자 변환\n",
    "    text = text.replace('.', '.')\n",
    "    words = text.split(' ')\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "            \n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "print(\"Corpus : \", corpus)\n",
    "print(\"word_to_id : \", word_to_id)\n",
    "print(\"id_to_word : \", id_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2 단어의 분산 표현\n",
    "색에는 고유한 이름이 붙여진 다채로운 색들도 있고, RGB(Red/Green/Blue)라는 세가지 성분이 어떤 비율로 섞여 있느냐로 표현하는 방법이 있다. 전자는 색의 가짓수만큼의 이름을 부여하는 반면에, 후자는 색을 3차원의 벡터로 표현한다. <br><br>\n",
    "\n",
    "여기서 주목할 점은 RGB같은 벡터 표현이 단 3개의 성분으로 간결하게 표현할 수 있고, 새글 더 정확하게 명시할 수 있다는 점이다. <br><br>\n",
    "\n",
    "'색'을 벡터로 표현하듯 '단어'도 벡터로 표현할 수 있다. 이를 단어의 __분산 표현__이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.3 분포 가설\n",
    "분포 가설이란 단어의 의미는 주변 단어에 의해 형성된다는 것이다. 분포 가설이 말하고자 하는 것은 단어 자체에는 의미가 없고, 그 단어가 사용된 __맥락__이 의미를 형성한다는 것이다. <br><br>\n",
    "\n",
    "예를 들어, __I drink beer__를 __I guzzle beer__라고 표현해도 guzzle을 drink로 이해할 수 있다는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.4 동시발생 행렬\n",
    "분포 가설에 기초애 한더를 벡터로 나타내는 방법을 생각해보면, 주변 단어를 세어보는 방법이 떠오를 것이며 이를 '통계 기반'기법이라고 한다.\n",
    "![](img/mat1.png)\n",
    "단어가 총 7개이며, 윈도우 크기는 1로 하고 단어 ID가 0인 'you'부터 단어의 맥락에 해당하는 단어 빈도를 세어보겠다. <br>\n",
    "'you'의 맥락은 'say'라는 단어 하나뿐이다.<br>\n",
    "표로 정리하면 밑에 그림과 같다.\n",
    "![](img/mat2.png)\n",
    "\n",
    "__'you'의 맥락으로써 동시에 발생하는 단어의 빈도를 나타낸 것이며, 벡터로 표현하면 [0, 1, 0, 0, 0, 0, 0] 이다.__ <br><br>\n",
    "그 다음 단어인 __say__로 같은 작업을 수행하면 밑의 그림과 같다.\n",
    "![](img/mat3.png)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "__'say'라는 단어는 벡터 [1, 0, 1, 0, 1, 1, 0]으로 표현할 수 있다.__ <br><br>\n",
    "\n",
    "![](img/mat4.png)\n",
    "<br>\n",
    "위의 표는 모든 단어에 대해 동시발생하는 단어를 표에 정리한 것이다. <br>\n",
    "위 표의 각 행은 벡터이며, 행렬의 형태를 띄워 __동시발생 행렬__이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 동시행렬 자동화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#말뭉치로부터 동시발생 행렬을 만들어주는 함수\n",
    "#자동화\n",
    "\n",
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    '''동시발생 행렬 생성\n",
    "    :param corpus: 말뭉치(단어 ID 목록)\n",
    "    :param vocab_size: 어휘 수\n",
    "    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)\n",
    "    :return: 동시발생 행렬\n",
    "    '''\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "\n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 1],\n",
       "       [0, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "create_co_matrix(corpus, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.5 벡터 간 유사도\n",
    "단어 벡터의 유사도를 나타낼 때는 __코사인 유사도__를 자주 이용한다.\n",
    "![](img/cosine1.png)\n",
    "\n",
    "위의 식처럼 정의되며, 분자에는 벡터의 내적, 분모에는 벡터의 노름(크기)가 등장한다. <br>\n",
    "위 식의 핵심은 벡터를 정규화하고 내적을 구하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    '''코사인 유사도 산출\n",
    "    :param x: 벡터\n",
    "    :param y: 벡터\n",
    "    :param eps: '0으로 나누기'를 방지하기 위한 작은 값\n",
    "    :return:\n",
    "    '''\n",
    "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.6 유사 단어의 랭킹 표시\n",
    "코사인 유사도를 이용하여 어떤 단어가 주어지면, 그 검색어와 비슷한 단어를 유사도 순으로 출력하는 함수를 만들어 본다. <br>\n",
    "이를 구현하기 위한 코드에는 밑 표의 인수들을 사용한다.\n",
    "\n",
    "![](img/most_similar.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    '''유사 단어 검색\n",
    "    :param query: 쿼리(텍스트)\n",
    "    :param word_to_id: 단어에서 단어 ID로 변환하는 딕셔너리\n",
    "    :param id_to_word: 단어 ID에서 단어로 변환하는 딕셔너리\n",
    "    :param word_matrix: 단어 벡터를 정리한 행렬. 각 행에 해당 단어 벡터가 저장되어 있다고 가정한다.\n",
    "    :param top: 상위 몇 개까지 출력할 지 지정\n",
    "    '''\n",
    "    if query not in word_to_id:\n",
    "        print('%s(을)를 찾을 수 없습니다.' % query)\n",
    "        return\n",
    "\n",
    "    print('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    vocab_size = len(id_to_word)\n",
    "\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "\n",
    "    # 코사인 유사도를 기준으로 내림차순으로 출력\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2.4 통계 기반 기법 개선하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4.1 상호정보량\n",
    "동시발생 행렬의 원소는 두 단어가 동시에 발생한 횟수를 나타내지만 __발생횟수__라는것은 사실 좋은 특징은 아니다. <br>\n",
    "예를 들어, __the__와 __car__의 동시발생을 생각해보자. __...the car...__라는 문구가 자주 보일 것이며, __car__와 __drive__는 관련이 깊다.<br> 하지만, __the__가 고빈도 단어이기 때문에 __car__와 더 관련이 있어 보이게 결과가 나올 수 있다. <br><br>\n",
    "\n",
    "이를 해결하기 위해 __점별 상호정보량(Pointwise Mutual Information : PMI)__라는 척도를 사용한다. PMI는 확률 변수 x와 y에 대해 다음 식으로 정의된다.\n",
    "![](img/pmi1.png)\n",
    "\n",
    "- $P(x)$는 $x$가 일어날 확률\n",
    "- $P(y)$는 $y$가 일어날 확률\n",
    "- $P(x,y)$는 $x,y$가 동시에 일어날 확률이다.\n",
    "\n",
    "__PMI값이 높을수록 관련성이 높다는 것이다.__ <br>\n",
    "\n",
    "위 식을 다시 정리하면 아래와 같다.\n",
    "![](img/pmi2.png)\n",
    "- $C$는 동시발생 행렬\n",
    "- $C(x,y)$는 단어 $x$와 $y$가 동시 발생하는 횟수\n",
    "- $C(x)$와 $C(y)$는 각각 단어 $x$와 $y$의 등장 횟수\n",
    "- $N$은 말뭉치에 포함된 단어 수\n",
    "\n",
    "위 식을 토대로 1,000번 등장한 'the', 20번 등장한 'car'와 10번 등장한 'drive를 계산해보자. <br>\n",
    "우선 __the__와 __car__의 동시발생 수가 10회라면, PMI 결과는 다음과 같다.\n",
    "![](img/pmi3.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "그 다음으로 __car__와 __drive__ 의 동시발생 수가 5라면 PMI 결과는 아래와 같다.\n",
    "\n",
    "![](img/pmi4.png)\n",
    "\n",
    "두 PMI의 결과를 살펴보면 __car__과 __drive__의 관계성이 강하다는 것을 볼 수 있다. 이러한 결과가 나온 이유는 단어가 단독으로 출현하는 횟수가 고려되었기 때문이다.  <br>\n",
    "이 예에서는 __the__가 자주 출현하였기 때문에 PMI값이 낮아진 것이다.<br><br>\n",
    "\n",
    "하지만 PMI에도 문제가 하나 있다. 이는 두 단어의 동시발생 횟수가 0이면 $log(0,2) = -\\inf$ 가 된다. <br>\n",
    "이 문제를 피하기 위해 실제 구현할 때는 __양의 상호정보량(PPMI)__를 사용한다.\n",
    "\n",
    "![](img/ppmi.png)\n",
    "\n",
    "이 식에 따라 PMI가 음수인 때는 0으로 취급하며 단어 사이의 관련성을 0 이상의 실수로 나타낼 수 있다. <br><br>\n",
    "\n",
    "하지만 PPMI 행렬에도 문제가 있는데 말뭉치의 어휘 수가 증가함에 따라 각 단어 벡터의 차원 수도 증가한다는 문제이다.<br><br>\n",
    "__이 문제를 대처해주고자 자주 수행하는 기법이 '벡터의 차원 감소'이다.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동시발생 행렬\n",
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n",
      "--------------------------------------------------\n",
      "PPMI\n",
      "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.807 0.    0.    0.    0.    2.807]\n",
      " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from common.util import preprocess, create_co_matrix, cos_similarity, ppmi\n",
    "\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "W = ppmi(C)\n",
    "\n",
    "np.set_printoptions(precision=3)  # 유효 자릿수를 세 자리로 표시\n",
    "print('동시발생 행렬')\n",
    "print(C)\n",
    "print('-'*50)\n",
    "print('PPMI')\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4.2 차원 감소\n",
    "\n",
    "__차원 감소__는 벡터의 차원을 '중요한 정보는 최대한 유지하면서 줄이는 방법이다.\n",
    "![](img/reduce_dim.png)\n",
    "\n",
    "위의 그림 예시처럼 데이터의 분포를 고려해 중요한 '축'을 찾는 일을 수행한다. 왼쪽 그림은 데이터점들을 2차원 좌표에 표시한 모습이고 오른쪽 그림은 새로운 축을 도입하여 똑같은 데이터를 촤표축 하나만으로 표시했다.<br><br>\n",
    "\n",
    "여기서 중요한 것은 가장 적합한 축을 찾아내는 일로, 1차원 값만으로 데이터의 본직적인 차이를 구별할 수 있어야 한다. 그리고 다차원 데이터에 대해서도 수행 가능하다.<br><br>\n",
    "\n",
    "차원을 감소시키는 방법 중 하나인 __특잇값분해(SVD)__는 임의의 행렬을 세 행렬의 곱으로 분해하며, 수식으로는 다음과 같다.\n",
    "\n",
    "$$\n",
    "X = USV^T\n",
    "$$\n",
    "- SVD는 임의의 행렬 $X$를 $U, S, V$라는 세 행렬의 곱으로 분해한다. <br>\n",
    "- $U$와 $V$는 직교행렬이고, 열벡터는 서로 직교한다 . <br>\n",
    "- $S$는 대각행렬이다. \n",
    "\n",
    "![](img/svd1.png)\n",
    "\n",
    "__※ 행렬 $S$에서 특잇값이 작다면 중요도가 낮다는 뜻이므로, 행렬 $U$에서 열벡터를 깎아내려 원래의 행렬을 근사할 수 있다.__\n",
    "\n",
    "![](img/svd2.png)\n",
    "\n",
    "이를 단어의 PPMI행렬에 적용하려면 행렬 $X$의 각 행에는 해당 단어ID의 단어 벡터가 저장되어 있다. <br>&nbsp;&nbsp; __==>>__그 단어 벡터가 행렬$U$ 라는 차원 감소된 벡터로 표현된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4.3 SVD에 의한 차원 감소\n",
    "- SVD는 넘파이의 linalg 모듈이 제공하는 svd메서드로 실행 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ptb.train.txt ... \n",
      "Done\n",
      "동시발생 수 계산 ...\n",
      "PPMI 계산 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Github\\deepLearning\\[2권] 순환 신경망과 자연어 처리\\common\\util.py:141: RuntimeWarning: overflow encountered in long_scalars\n",
      "  pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
      "c:\\Github\\deepLearning\\[2권] 순환 신경망과 자연어 처리\\common\\util.py:141: RuntimeWarning: invalid value encountered in log2\n",
      "  pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0% 완료\n",
      "2.0% 완료\n",
      "3.0% 완료\n",
      "4.0% 완료\n",
      "5.0% 완료\n",
      "6.0% 완료\n",
      "7.0% 완료\n",
      "8.0% 완료\n",
      "9.0% 완료\n",
      "10.0% 완료\n",
      "11.0% 완료\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-8c6556bf077e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_co_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'PPMI 계산 ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mppmi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'calculating SVD ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Github\\deepLearning\\[2권] 순환 신경망과 자연어 처리\\common\\util.py\u001b[0m in \u001b[0;36mppmi\u001b[1;34m(C, verbose, eps)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0mpmi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mN\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m             \u001b[0mM\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpmi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from common.util import most_similar, create_co_matrix, ppmi\n",
    "from dataset import ptb\n",
    "\n",
    "\n",
    "window_size = 2\n",
    "wordvec_size = 100\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "print('동시발생 수 계산 ...')\n",
    "C = create_co_matrix(corpus, vocab_size, window_size)\n",
    "print('PPMI 계산 ...')\n",
    "W = ppmi(C, verbose=True)\n",
    "\n",
    "print('calculating SVD ...')\n",
    "try:\n",
    "    # truncated SVD (빠르다!)\n",
    "    from sklearn.utils.extmath import randomized_svd\n",
    "    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5,\n",
    "                             random_state=None)\n",
    "except ImportError:\n",
    "    # SVD (느리다)\n",
    "    U, S, V = np.linalg.svd(W)\n",
    "\n",
    "word_vecs = U[:, :wordvec_size]\n",
    "\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과적으로 말뭉치를 사용해 맥락에 속한 단어의 등장 횟수를 센 후 PPMI 행렬로 변환하고 다시 SVD를 이용해 차원을 감소시킴으로서 더 좋은 단어 벡터를 얻었다. <br><br>\n",
    "이것이 단어의 분산 표현이고, 각 단어는 고정 길이의 밀집벡터로 표현되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
