{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고 링크\n",
    "- https://koreanfoodie.me/157\n",
    "- https://nbviewer.jupyter.org/github/SDRLurker/deep-learning/blob/master/5%E1%84%8C%E1%85%A1%E1%86%BC.ipynb\n",
    "- 밑바닥부터 시작하는 딥러닝1 책"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 오차 역전파법 구현하기\n",
    "\n",
    "#### 5.7.1 신경망 학습의 전체 그림\n",
    "##### 전제\n",
    "__학습__ : 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정 <br>\n",
    "\n",
    "##### 1단계 - 미니배치\n",
    "__미니배치 :__ 훈련 데이터 중 일부를 무작위로 가져옴 <br>\n",
    "__목표 :__ 미니배치의 손실 함수 값을 줄이기\n",
    "<br><br>\n",
    "\n",
    "##### 2단계 - 기울기 산출\n",
    "가중치 매개변수의 기울기를 구함 => 기울기는 __손실함수의 값을 가장 작게하는 방향을 제시__\n",
    "<br><br>\n",
    "\n",
    "##### 3단계 - 매개변수 갱신\n",
    "가중치 매개변수를 기울기 방향으로 아주 조금 갱신\n",
    "<br><br>\n",
    "\n",
    "##### 4단계 - 반복\n",
    "- 1~3단계를 반복<br>\n",
    "- <u>오차역전법이 등장하는 단계는 두번째인 __기울기 산출__</u>\n",
    "- <u>느린 수치 미분과 달리, __기울기를 효율적이고 빠르게__ 구할 수 있음</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.7.2 오차역전파법을 적용한 신경망 구현하기\n",
    "\n",
    "계층을 사용함으로써 __predict()__와 기울기를 구하는 __gradient()__ 계층의 전파만으로 동작이 이뤄진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/mnist-affine.png)\n",
    "__위와 같은 Layer를 갖는 Neural Net구현하기__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/WegraLee/deep-learning-from-scratch/blob/master/ch05/two_layer_net.py 참고\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성 \n",
    "        self.layers = OrderedDict()                                           ###\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1']) ###\n",
    "        self.layers['Relu1'] = Relu()                                         ###\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2']) ###\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()                                    ###\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():                                    ###\n",
    "            x = layer.forward(x)                                              ###\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)                      ###\n",
    "\n",
    "        # backward\n",
    "        dout = 1                             ###\n",
    "        dout = self.lastLayer.backward(dout) ###\n",
    "        \n",
    "        layers = list(self.layers.values())  ###\n",
    "        layers.reverse()                     ###\n",
    "        for layer in layers:                 ###\n",
    "            dout = layer.backward(dout)      ###\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __\\### 으로 중요코드 표시__\n",
    "> - 순전파 때는 추가한 순서대로 각 계층의 forward() 메서드를 호출\n",
    "> - 역전파 때는 계층을 반대 순서로 호출\n",
    "> - 신경망의 구성 요소를 '계층'으로 구현한 덕분에 신경망을 쉽게 구축\n",
    ">     - __=>__ 레고 블록을 조립하듯 필요한 만큼 계층을 더 추가하면 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.7.3 오차역전파법으로 구한 기울기 검증하기\n",
    "- 수치미분은 매우 느리다는것을 보여주기 위해 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.56 s ± 893 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit network.numerical_gradient(x_batch, t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277 µs ± 29.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit network.gradient(x_batch, t_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 수치미분(numerical_gradient) 속도: 9.56초\n",
    "> - 오차역전법(gradient) 속도: 277 µs, 0.000277초\n",
    "> - 약 34,000배 속도차이가 남\n",
    "> - __수치 미분을 구현하긴 쉽지만, 속도가 오차역전법(gradient)에 비해 매우 느림__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.7.4 오차역전파법을 사용한 학습 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09948333333333333 0.1024\n",
      "0.9026 0.9056\n",
      "0.9217333333333333 0.9243\n",
      "0.93285 0.9336\n",
      "0.9426 0.9402\n",
      "0.9510333333333333 0.9486\n",
      "0.9542833333333334 0.9518\n",
      "0.9607833333333333 0.9552\n",
      "0.9645333333333334 0.9576\n",
      "0.9671 0.9605\n",
      "0.9708833333333333 0.9655\n",
      "0.9727 0.9648\n",
      "0.97505 0.9665\n",
      "0.9760333333333333 0.9677\n",
      "0.9763833333333334 0.9676\n",
      "0.9788833333333333 0.967\n",
      "0.9787833333333333 0.9704\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "#from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
    "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 정리\n",
    "- 계산 그래프를 이용하여 신경망의 동작과 오차역전파법을 설명\n",
    "- 모든 계층에서 forward와 backward 메서드를 구현\n",
    "    - => __가중치 매개변수의 기울기를 효율적으로 구할 수 있음__\n",
    "    \n",
    "##### 이번 장에서 배운 것\n",
    "- 계산 그래프를 이용하면 계산 과정을 시각적으로 파악 가능\n",
    "- __오차역전파법 : 신경망의 구성 요소를 계층으로 구현하여 기울기를 효율적으로 계산__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
