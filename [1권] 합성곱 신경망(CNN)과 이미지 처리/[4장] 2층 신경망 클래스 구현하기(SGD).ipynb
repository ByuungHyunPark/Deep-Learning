{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고 링크 : https://nbviewer.jupyter.org/github/SDRLurker/deep-learning/blob/master/4%EC%9E%A5.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <4.5 신경망 학습 알고리즘 구현하기>\n",
    "전제 : 신경망에는 __적응 가능한 가중치와 편향__이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다. 신경망 학습은 다음과 같이 4단계로 수행한다.\n",
    "\n",
    "---\n",
    "##### 1단계 - 미니 배치\n",
    "훈련 데이터 중 일부를 무작위로 가져온다.<br>\n",
    "선별한 데이터를 미니배치라 하며, 그 __미니배치의 손실 함수 값을 줄이는 것이 목표__이다.\n",
    "\n",
    "##### 2단계 - 기울기 산출\n",
    "미니배치의 손실 함수 값을 줄이기 위해 __각 가중치 매개변수의 기울기를 구한다.__<br>\n",
    "기울기는 솔실 함수의 값을 가장 작게 하는 방향을 제시한다.\n",
    "\n",
    "###### 3단계 - 매개변수 갱신\n",
    "가중치 매개변수 기울기 방향으로 조금씩 갱신한다.\n",
    "\n",
    "##### 4단계 - 반복\n",
    "1~3단계를 반복한다.\n",
    "\n",
    "---\n",
    "이는 경사 하강법으로 매개변수를 갱신하는 방법이며, __이때 데이터를 미니배치로 무작위로 선정하기 때문에 확률 경사 하강법(Stochastic gradient descnet, SGD)__라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <4.5.1 2층 신경망 클래스 구현하기>\n",
    "\n",
    "2층 신경망을 하나의 클래스로 구현. 클래스 명은 TwoLayerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 교차 엔트로피 교차\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
    "\n",
    "\n",
    "# 가중치 매개변수의 기울기 구함\n",
    "def numerical_gradient(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_no_batch(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in tqdm(enumerate(X)):\n",
    "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
    "        \n",
    "        return grad\n",
    "    \n",
    "# 기울기 구현\n",
    "def _numerical_gradient_no_batch(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) \n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_diff(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def 시그모이드_접선(x): # 접선 ax+b에서 a,b 값을 리턴\n",
    "    return sigmoid_diff(x), sigmoid(x) - sigmoid_diff(x) * x\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/WegraLee/deep-learning-from-scratch/blob/master/ch04/two_layer_net.py 소스 참고\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
    "\n",
    "class TwoLayerNet:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        #가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "\n",
    "        return cross_entropy_error(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        t = np.argmax(t, axis = 1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # x : 입력데이터,  t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b2'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis = 0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b2'] = np.sum(dz1, axis = 0)\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape) # (784, 100)\n",
    "print(net.params['b1'].shape) # (100,)\n",
    "print(net.params['W2'].shape) # (100, 10)\n",
    "print(net.params['b2'].shape) # (10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100, 784) # 더미 입력 데이터(100장 분량)\n",
    "y = net.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "784it [02:31,  5.16it/s]\n",
      "100it [00:01, 50.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100, 784) # 더미 입력 데이터(100장 분량)\n",
    "t = np.random.rand(100, 10)  # 더미 정답 레이블(100장 분량)\n",
    "\n",
    "grads = net.numerical_gradient(x, t) # 기울기 계산\n",
    "\n",
    "print(grads['W1'].shape) # (784, 100)\n",
    "print(grads['b1'].shape) # (100,)\n",
    "print(grads['W2'].shape) # (100, 10)\n",
    "print(grads['b2'].shape) # (10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## [예제] mnist예제를 활용한 2층 신경망 클래스로 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__STEP01 : Data Load & Hyper parameter setting__\n",
    "- 미니배치 크기 : 100\n",
    "- iteration만큼 갱신을 하며, 갱신 때 마다 손실 함수를 계산하고, 그 값을 배열에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "\n",
    "print(network.params['W1'].shape) # (784, 100)\n",
    "print(network.params['b1'].shape) # (100,)\n",
    "print(network.params['W2'].shape) # (100, 10)\n",
    "print(network.params['b2'].shape) # (10,)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 5  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__STEP02 : Training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# iter_num :  0\n",
      "(100, 784) (100, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "784it [02:58,  4.38it/s]\n",
      "100it [00:02, 41.70it/s]\n",
      "1it [00:00,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.09915, 0.1009\n",
      "# iter_num :  1\n",
      "(100, 784) (100, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "784it [03:02,  4.30it/s]\n",
      "100it [00:02, 41.57it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# iter_num :  2\n",
      "(100, 784) (100, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "784it [03:06,  4.21it/s]\n",
      "100it [00:02, 40.02it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# iter_num :  3\n",
      "(100, 784) (100, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "784it [03:02,  4.30it/s]\n",
      "100it [00:02, 40.73it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# iter_num :  4\n",
      "(100, 784) (100, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "784it [03:01,  4.31it/s]\n",
      "100it [00:02, 41.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# iter만큼 학습 과정\n",
    "# iteration 책 예제에서는 10000인데, 시간 너무 오래걸리므로 상당히 많이 줄임. \n",
    "\n",
    "for i in range(iters_num):\n",
    "    print('# iter_num : ', i+1)\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    print(x_batch.shape, t_batch.shape)\n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    #grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__한 epoch 도는데 너무 오래걸림,, 대략적인 흐름만 파악하고, 아래는 epoch 당, loss값과 정확도를 통해 성능 향상 파악__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 3.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEKCAYAAAAYd05sAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWBklEQVR4nO3df4xdZZ3H8c+nZRDSgqzbSewC0jWyNtUsPxxqCashhLCIxG5W5McuuEFMVxPdsroSUKPxPzYkRDRE0wgqsdYYC4Y0YMUI2+BKy0xtS2upQVZjI24LLKUVRUu/+8c5095759yZMzP3uefO0/crmXDPOc95nu/9cvqdM+ee81xHhAAA+ZnXdAAAgDQo8ACQKQo8AGSKAg8AmaLAA0CmKPAAkKlkBd72Sba32N5ue5ftL6QaCwAwkVPdB2/bkhZExCHbQ5Iel7Q6Ip5IMiAAoM0JqTqO4jfHoXJxqPzhqSoA6JNkBV6SbM+XNCbpLZLujojNFW1WSVolSQsWLHjH0qVLU4YEAFkZGxt7PiKGq7Ylu0TTNoh9mqQHJH08InZ2azcyMhKjo6PJ4wGAXNgei4iRqm19uYsmIl6S9Jiky/sxHgAg7V00w+WZu2yfLOlSSU+nGg8A0C7lNfjFkr5ZXoefJ+m7EbEh4XgAgBYp76LZIem8VP0DACbHk6wAkCkKPABkigIPAJmiwANApijwAJApCjwAZIoCDwCZosADQKYo8ACQKQo8AGSKAg8AmaLAA0CmKPAAkCkKPABkigIPAJmiwANAplJ+oxMGREQoQorydTe2J66rbDf1fgCal0WB/5d7t+jVw68dLWIKKdRe1Ir/lttVLBxbV7btKIKt28puj/alyr6P9XNsmIrtGm9THaPqjtPyXo/FO7GvQVH1e6Bz1Ux/yRTtOn/zTD1e3b5a2/joOrf36S7bu6xvX9e+ZuI+7XFNFk9nzN32nSyeru+lRjzq2GfQDGJUf7FgSGs/vKLn/WZR4F87EjpyRJKL/3meJ1nzZBcHnuWWg91tB6lb1hVt3LL+2L5tB3TFfq3L6hyzsq8u46jjH9yEbdXjuEt7tcXY3rZV1e+Cql8Q0dGyuk29zjrX1Bmv7ph1+6qzqvWvntZf9u3L0bbcuW9n+6p9Jrbp2N4xVp14NGH7ZPFUt1GX9zjZex+wc4ujBu2kZ9ypJ6cpxVkU+G99+J1NhwAAA4cPWQEgUxR4AMgUBR4AMkWBB4BMJSvwts+0/ajt3bZ32V6daiwAwEQp76I5LOmTEbHV9imSxmw/EhE/TzgmAKCU7Aw+Ip6LiK3l64OSdks6PdV4AIB2fbkGb3uJpPMkba7Ytsr2qO3R/fv39yMcADguJC/wthdKWi/p5oh4uXN7RKyJiJGIGBkeHk4dDgAcN5IWeNtDKor72oi4P+VYAIB2Ke+isaR7JO2OiDtTjQMAqJbyDP4iSTdIusT2tvLnioTjAQBaJLtNMiIe12DOzAkAxwWeZAWATFHgASBTFHgAyBQFHgAyVavA215t+1QX7rG91fZlqYMDAMxc3TP4D5VPoV4maVjSjZJuTxYVAGDW6hb48dsdr5D09YjYLm6BBICBVrfAj9n+oYoCv7Gc/vdIurAAALNV90GnmySdK+nZiHjF9htUXKYBAAyoumfwF0raExEv2b5e0mclHUgXFgBgtuoW+K9IesX2OZJukfRrSfcliwoAMGt1C/zhiAhJKyXdFRF3STolXVgAgNmqew3+oO3bVMwO+S7b8yUNpQsLADBbdc/gr5H0qor74X+n4rtV70gWFQBg1moV+LKor5X0ettXSvpjRHANHgAGWN2pCq6WtEXSByRdLWmz7atSBgYAmJ261+A/I+mCiNgnSbaHJf1I0vdSBQYAmJ261+DnjRf30gvT2BcA0IC6Z/A/sL1R0rpy+RpJD6UJCQDQC7UKfER8yvb7VXyRtiWtiYgHkkYGAJiV2l+6HRHrJa1PGAsAoIcmLfC2D0qKqk2SIiJOTRIVAGDWJi3wEcF0BAAwR3EnDABkKlmBt32v7X22d6YaAwDQXcoz+G9Iujxh/wCASSQr8BGxSdKLqfoHAEyu8WvwtlfZHrU9un///qbDAYBsNF7gI2JNRIxExMjw8HDT4QBANhov8ACANCjwAJCplLdJrpP0U0lvtb3X9k2pxgIATFR7LprpiojrUvUNAJgal2gAIFMUeADIFAUeADJFgQeATFHgASBTFHgAyBQFHgAyRYEHgExR4AEgUxR4AMgUBR4AMkWBB4BMUeABIFMUeADIFAUeADJFgQeATFHgASBTFHgAyBQFHgAyRYEHgExR4AEgUxR4AMgUBR4AMkWBB4BMJS3wti+3vcf2M7ZvTTkWAKBdsgJve76kuyW9R9IySdfZXpZqPABAu5Rn8MslPRMRz0bEnyR9R9LKhOMBAFqckLDv0yX9pmV5r6R3djayvUrSqnLxkO09MxxvkaTnZ7hvSsQ1PcQ1PcQ1PTnGdVa3DSkLvCvWxYQVEWskrZn1YPZoRIzMtp9eI67pIa7pIa7pOd7iSnmJZq+kM1uWz5D024TjAQBapCzwT0o62/Zf2z5R0rWSHkw4HgCgRbJLNBFx2PbHJG2UNF/SvRGxK9V46sFlnkSIa3qIa3qIa3qOq7gcMeGyOAAgAzzJCgCZosADQKbmVIGfauoDF75Ubt9h+/wBieti2wdsbyt/PtenuO61vc/2zi7bm8rXVHE1la8zbT9qe7ftXbZXV7Tpe85qxtX3nNk+yfYW29vLuL5Q0aaJfNWJq5FjrBx7vu2f2d5Qsa23+YqIOfGj4oPaX0p6s6QTJW2XtKyjzRWSHlZxD/4KSZsHJK6LJW1oIGfvlnS+pJ1dtvc9XzXjaipfiyWdX74+RdIvBuQYqxNX33NW5mBh+XpI0mZJKwYgX3XiauQYK8f+hKRvV43f63zNpTP4OlMfrJR0XxSekHSa7cUDEFcjImKTpBcnadJEvurE1YiIeC4itpavD0rareKJ7FZ9z1nNuPquzMGhcnGo/Om8a6OJfNWJqxG2z5D0Xklf69Kkp/maSwW+auqDzoO8Tpsm4pKkC8s/GR+2/bbEMdXVRL7qajRftpdIOk/F2V+rRnM2SVxSAzkrLzdsk7RP0iMRMRD5qhGX1Mwx9kVJt0g60mV7T/M1lwp8nakPak2P0GN1xtwq6ayIOEfSlyV9P3FMdTWRrzoazZfthZLWS7o5Il7u3FyxS19yNkVcjeQsIl6LiHNVPKm+3PbbO5o0kq8acfU9X7avlLQvIsYma1axbsb5mksFvs7UB01MjzDlmBHx8vifjBHxkKQh24sSx1XHQE4n0WS+bA+pKKJrI+L+iiaN5GyquJo+xiLiJUmPSbq8Y1Ojx1i3uBrK10WS3mf7Vyou5V5i+1sdbXqar7lU4OtMffCgpA+Wn0SvkHQgIp5rOi7bb7Tt8vVyFXl/IXFcdTSRryk1la9yzHsk7Y6IO7s063vO6sTVRM5sD9s+rXx9sqRLJT3d0ayJfE0ZVxP5iojbIuKMiFiiok78OCKu72jW03ylnE2yp6LL1Ae2P1Ju/6qkh1R8Cv2MpFck3TggcV0l6aO2D0v6g6Rro/zIPCXb61TcLbDI9l5Jn1fxgVNj+aoZVyP5UnGGdYOkp8rrt5L0aUlvaomtiZzViauJnC2W9E0XX+4zT9J3I2JD0/8ma8bV1DE2Qcp8MVUBAGQq5Vf2TfmwAQAgnWRn8OX1rQURcaj8gOhxSavLezsBAImlnC44JA3kwwYAcDxI+iFr+SHHmKS3SLq76mEDt3wn64IFC96xdOnSlCEBQFbGxsaej4jhqm19+ZC1vGXpAUkfj4jKCaYkaWRkJEZHR5PHAwC5sD0WXb7PtS/3wU/yEAQAIJGUd9HUeQgCAJBIymvwlQ8bJBwPANAi5V00O1TMegcAaMBcmosGADANFHgAyBQFHgAyRYEHgExR4AEgUxR4AMgUBR4AMkWBB4BMUeABIFMUeADIFAUeADJFgQeATFHgASBTFHgAyBQFHgAyRYEHgEyl/Eanvvn1C7/XkZAiQiGp+B7xUISOLsf4cvkd4+PL6tyu6n6KV2Xblu2hYqcJ42i876KtOmLoNs6x/SbGNGF957bOcapimuxL1u32xambyBWtJraZWT+dq6r7mUHMlUN5yjadY7qj7Xgfrft2tlFHm2Pb3XWfzv47/jOtmNre1hRt2mKasG3y99O5fyrpRyjHSTzQCfPmadlfndr7fnveYwP+/oub9Mc/H2k6DACYkUULX6fRz17a836zKPD/+f6/1ZEIWW456ynOS2wdXT++3HrW4c62ZXtVbCv2PNaXJvQ9jXHaYm3pZ3z8tv3Kkafo5+g+Xd67KtqP6zyxD1Wc6U9oM7N+Jrap6ic6lisaTdFv1fjVbaYYu3L/aFuOCduPjd36l2LV+qr4JuzbEdtMYmp9XxO2qX2nqv6nE1Pl/9QeqzxGU4zTh2GG5qe5Wp5FgV957ulNhwAAA4cPWQEgUxR4AMgUBR4AMkWBB4BMJSvwts+0/ajt3bZ32V6daiwAwEQp76I5LOmTEbHV9imSxmw/EhE/TzgmAKCU7Aw+Ip6LiK3l64OSdkvifkYA6JO+XIO3vUTSeZI2V2xbZXvU9uj+/fv7EQ4AHBeSF3jbCyWtl3RzRLzcuT0i1kTESESMDA8Ppw4HAI4bSQu87SEVxX1tRNyfciwAQLuUd9FY0j2SdkfEnanGAQBUS3kGf5GkGyRdYntb+XNFwvEAAC2S3SYZEY+rf9M1AwA68CQrAGSKAg8AmaLAA0CmKPAAkKlaBd72atununCP7a22L0sdHABg5uqewX+ofAr1MknDkm6UdHuyqAAAs1a3wI/f7niFpK9HxHZxCyQADLS6BX7M9g9VFPiN5fS/R9KFBQCYrboPOt0k6VxJz0bEK7bfoOIyDQBgQNU9g79Q0p6IeMn29ZI+K+lAurAAALNVt8B/RdIrts+RdIukX0u6L1lUAIBZq1vgD0dESFop6a6IuEvSKenCAgDMVt1r8Adt36Zidsh32Z4vaShdWACA2ap7Bn+NpFdV3A//OxXfrXpHsqgAALNWq8CXRX2tpNfbvlLSHyOCa/AAMMDqTlVwtaQtkj4g6WpJm21flTIwAMDs1L0G/xlJF0TEPkmyPSzpR5K+lyowAMDs1L0GP2+8uJdemMa+AIAG1D2D/4HtjZLWlcvXSHooTUgAgF6oVeAj4lO236/ii7QtaU1EPJA0MgDArNT+0u2IWC9pfcJYAAA9NGmBt31QUlRtkhQRcWqSqAAAszZpgY8IpiMAgDmKO2EAIFPJCrzte23vs70z1RgAgO5SnsF/Q9LlCfsHAEwiWYGPiE2SXkzVPwBgco1fg7e9yvao7dH9+/c3HQ4AZKPxAh8RayJiJCJGhoeHmw4HALLReIEHAKRBgQeATKW8TXKdpJ9KeqvtvbZvSjUWAGCi2nPRTFdEXJeqbwDA1LhEAwCZosADQKYo8ACQKQo8AGSKAg8AmaLAA0CmKPAAkCkKPABkigIPAJmiwANApijwAJApCjwAZIoCDwCZosADQKYo8ACQKQo8AGSKAg8AmaLAA0CmKPAAkCkKPABkigIPAJmiwANApijwAJApCjwAZCppgbd9ue09tp+xfWvKsQAA7ZIVeNvzJd0t6T2Slkm6zvayVOMBANqlPINfLumZiHg2Iv4k6TuSViYcDwDQ4oSEfZ8u6Tcty3slvbOzke1VklaVi4ds75nheIskPT/DfVMirukhrukhrunJMa6zum1IWeBdsS4mrIhYI2nNrAezRyNiZLb99BpxTQ9xTQ9xTc/xFlfKSzR7JZ3ZsnyGpN8mHA8A0CJlgX9S0tm2/9r2iZKulfRgwvEAAC2SXaKJiMO2PyZpo6T5ku6NiF2pxlMPLvMkQlzTQ1zTQ1zTc1zF5YgJl8UBABngSVYAyBQFHgAyNacK/FRTH7jwpXL7DtvnD0hcF9s+YHtb+fO5PsV1r+19tnd22d5UvqaKq6l8nWn7Udu7be+yvbqiTd9zVjOuvufM9km2t9jeXsb1hYo2TeSrTlyNHGPl2PNt/8z2hoptvc1XRMyJHxUf1P5S0pslnShpu6RlHW2ukPSwinvwV0jaPCBxXSxpQwM5e7ek8yXt7LK97/mqGVdT+Vos6fzy9SmSfjEgx1iduPqeszIHC8vXQ5I2S1oxAPmqE1cjx1g59ickfbtq/F7nay6dwdeZ+mClpPui8ISk02wvHoC4GhERmyS9OEmTJvJVJ65GRMRzEbG1fH1Q0m4VT2S36nvOasbVd2UODpWLQ+VP510bTeSrTlyNsH2GpPdK+lqXJj3N11wq8FVTH3Qe5HXaNBGXJF1Y/sn4sO23JY6pribyVVej+bK9RNJ5Ks7+WjWas0nikhrIWXm5YZukfZIeiYiByFeNuKRmjrEvSrpF0pEu23uar7lU4OtMfVBreoQeqzPmVklnRcQ5kr4s6fuJY6qriXzV0Wi+bC+UtF7SzRHxcufmil36krMp4mokZxHxWkScq+JJ9eW2397RpJF81Yir7/myfaWkfRExNlmzinUzztdcKvB1pj5oYnqEKceMiJfH/2SMiIckDdlelDiuOgZyOokm82V7SEURXRsR91c0aSRnU8XV9DEWES9JekzS5R2bGj3GusXVUL4ukvQ+279ScSn3Etvf6mjT03zNpQJfZ+qDByV9sPwkeoWkAxHxXNNx2X6jbZevl6vI+wuJ46qjiXxNqal8lWPeI2l3RNzZpVnfc1YnriZyZnvY9mnl65MlXSrp6Y5mTeRryriayFdE3BYRZ0TEEhV14scRcX1Hs57mK+Vskj0VXaY+sP2RcvtXJT2k4lPoZyS9IunGAYnrKkkftX1Y0h8kXRvlR+Yp2V6n4m6BRbb3Svq8ig+cGstXzbgayZeKM6wbJD1VXr+VpE9LelNLbE3krE5cTeRssaRvuvhyn3mSvhsRG5r+N1kzrqaOsQlS5oupCgAgU3PpEg0AYBoo8ACQKQo8AGSKAg8AmaLAA0CmKPDIku3/Lv+7xPY/9bjvT1eNBQwabpNE1mxfLOk/IuLKaewzPyJem2T7oYhY2IPwgKQ4g0eWbI/PJni7pHe5mPP738tJqO6w/aSL+bb/tWx/sYs5178t6aly3fdtj7mYU3xVue52SSeX/a1tHat8+vAO2zttP2X7mpa+H7P9PdtP2147/hQlkNKceZIVmKFb1XIGXxbqAxFxge3XSfqJ7R+WbZdLentE/E+5/KGIeLF83P1J2+sj4lbbHysnsur0j5LOlXSOpEXlPpvKbedJepuKeUV+ouLp1Md7/WaBVpzB43hzmYq5PrapmHL3LyWdXW7b0lLcJenfbG+X9ISKCaDO1uT+TtK6cibD/5X0X5IuaOl7b0QckbRN0pIevBdgUpzB43hjSR+PiI1tK4tr9b/vWL5U0oUR8YrtxySdVKPvbl5tef2a+LeHPuAMHrk7qOJr7sZtVDHJ1JAk2f4b2wsq9nu9pP8ri/tSFV+fNu7P4/t32CTpmvI6/7CKrybc0pN3AcwAZxHI3Q5Jh8tLLd+QdJeKyyNbyw8690v6h4r9fiDpI7Z3SNqj4jLNuDWSdtjeGhH/3LL+AUkXqvhe3pB0S0T8rvwFAfQdt0kCQKa4RAMAmaLAA0CmKPAAkCkKPABkigIPAJmiwANApijwAJCp/weAhYTFfUSWkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 그림 4-11 손실 함수의 추이: 위쪽은 10,000회 반복까지의 추이, 아래쪽은 1,000회 반복까지의 추이\n",
    "f, (ax1, ax2) = plt.subplots(2, 1)\n",
    "x = np.array(range(iters_num))\n",
    "ax1.plot(x, train_loss_list, label='loss')\n",
    "ax1.set_xlabel(\"iteration\")\n",
    "ax1.set_ylabel(\"loss\")\n",
    "ax1.set_ylim(0, 3.0)\n",
    "ax2.plot(x[:1000], train_loss_list[:1000], label='loss')\n",
    "ax2.set_xlabel(\"iteration\")\n",
    "ax2.set_ylabel(\"loss\")\n",
    "ax2.set_ylim(0, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
